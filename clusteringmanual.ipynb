{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***GENERATED CODE FOR clusteringmanual PIPELINE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DON'T EDIT THIS CODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CONNECTOR FUNCTIONS TO READ DATA.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***AUTOML FUNCTIONS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def meanshift(df, TrainingPercent):\n",
    "    rows = df.count()\n",
    "    percentrow = TrainingPercent / 100 * rows\n",
    "    percentrows = int(percentrow)\n",
    "    df = df.limit(percentrows)\n",
    "    samples = int(percentrows/4)\n",
    "    pd_df = df.toPandas()\n",
    "    bandwidth = estimate_bandwidth(pd_df, n_samples=samples)\n",
    "    model = MeanShift(bandwidth=bandwidth).fit(pd_df)\n",
    "    labels = model.predict(pd_df)\n",
    "    silhouette = metrics.silhouette_score(pd_df, labels, metric='euclidean')\n",
    "    pd_df['prediction'] = pd.DataFrame(labels)\n",
    "    labels_unique = np.unique(labels)\n",
    "    totalClusters = len(labels_unique)\n",
    "    display(\"totalClusters     : %s\" % totalClusters)\n",
    "    display(pd_df.head())\n",
    "    return pd_df\n",
    "\n",
    "\n",
    "class Clustering:\n",
    "\n",
    "    def run(spark_DF, spark, config):\n",
    "        stageAttributes = json.loads(config)\n",
    "        stageAttributes['model']\n",
    "        trainingPercent = stageAttributes['TrainingPercent']\n",
    "        kmeans_model = meanshift(df=spark_DF, TrainingPercent=trainingPercent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***READING DATAFRAME***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run clusteringmanualHooks.ipynb\n",
    "try:\n",
    "\t\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TRAIN MODEL***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run clusteringmanualHooks.ipynb\n",
    "try:\n",
    "\t#mlPreExecutionHook()\n",
    "\n",
    "\tclustering = Clustering.run(clustering,spark,json.dumps( {\"autoClustering\": 0, \"originalfile\": \"/FileStore/platform/Data/TrainData/testing/clustering.csv\", \"model\": \"meanshift\", \"TrainingPercent\": 100, \"totalClusters\": null}))\n",
    "\n",
    "\t#mlPostExecutionHook(clustering)\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
